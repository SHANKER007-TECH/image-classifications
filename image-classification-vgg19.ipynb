{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:37.343197Z","iopub.execute_input":"2024-12-29T05:58:37.343608Z","iopub.status.idle":"2024-12-29T05:58:37.720560Z","shell.execute_reply.started":"2024-12-29T05:58:37.343469Z","shell.execute_reply":"2024-12-29T05:58:37.719569Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Install And Imprort Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, AveragePooling2D,MaxPooling2D,BatchNormalization,Dropout,GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:37.721556Z","iopub.execute_input":"2024-12-29T05:58:37.722003Z","iopub.status.idle":"2024-12-29T05:58:46.974542Z","shell.execute_reply.started":"2024-12-29T05:58:37.721959Z","shell.execute_reply":"2024-12-29T05:58:46.973635Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Read the Dataset","metadata":{}},{"cell_type":"code","source":"# loading the directories\n\ntraining_dir = '/kaggle/input/fruits/fruits-360_dataset_100x100/fruits-360/Training'\ntest_dir = '/kaggle/input/fruits/fruits-360_dataset_100x100/fruits-360/Test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:46.975383Z","iopub.execute_input":"2024-12-29T05:58:46.975912Z","iopub.status.idle":"2024-12-29T05:58:46.980263Z","shell.execute_reply.started":"2024-12-29T05:58:46.975885Z","shell.execute_reply":"2024-12-29T05:58:46.978982Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# useful for getting number of files\nimage_files = glob(training_dir + '/*/*.jp*g')\ntest_image_files = glob(test_dir + '/*/*.jp*g')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:46.982346Z","iopub.execute_input":"2024-12-29T05:58:46.982674Z","iopub.status.idle":"2024-12-29T05:58:46.999489Z","shell.execute_reply.started":"2024-12-29T05:58:46.982649Z","shell.execute_reply":"2024-12-29T05:58:46.998275Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#image_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.001300Z","iopub.execute_input":"2024-12-29T05:58:47.001693Z","iopub.status.idle":"2024-12-29T05:58:47.010324Z","shell.execute_reply.started":"2024-12-29T05:58:47.001644Z","shell.execute_reply":"2024-12-29T05:58:47.008935Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#test_image_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.011821Z","iopub.execute_input":"2024-12-29T05:58:47.012256Z","iopub.status.idle":"2024-12-29T05:58:47.025818Z","shell.execute_reply.started":"2024-12-29T05:58:47.012215Z","shell.execute_reply":"2024-12-29T05:58:47.024784Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# getting the number of classes i.e. type of fruits\nfolders = glob(training_dir + '/*')\nnum_classes = len(folders)\nprint ('Total Classes = ' + str(num_classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.026938Z","iopub.execute_input":"2024-12-29T05:58:47.027256Z","iopub.status.idle":"2024-12-29T05:58:47.044037Z","shell.execute_reply.started":"2024-12-29T05:58:47.027227Z","shell.execute_reply":"2024-12-29T05:58:47.042824Z"}},"outputs":[{"name":"stdout","text":"Total Classes = 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Read Data and Data Augumentation","metadata":{}},{"cell_type":"code","source":"# Image Augmentation\nfrom keras.applications import VGG19\nfrom keras.utils import image_dataset_from_directory\nfrom keras.applications.vgg16 import preprocess_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.045430Z","iopub.execute_input":"2024-12-29T05:58:47.045984Z","iopub.status.idle":"2024-12-29T05:58:47.059460Z","shell.execute_reply.started":"2024-12-29T05:58:47.045934Z","shell.execute_reply":"2024-12-29T05:58:47.058296Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.060709Z","iopub.execute_input":"2024-12-29T05:58:47.061384Z","iopub.status.idle":"2024-12-29T05:58:47.076080Z","shell.execute_reply.started":"2024-12-29T05:58:47.061330Z","shell.execute_reply":"2024-12-29T05:58:47.075031Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/fruits/fruits-360_dataset_100x100/fruits-360/Test'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"training_generator = image_dataset_from_directory(training_dir,image_size=(224,224),batch_size =200,label_mode=\"categorical\")\ntraining_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.077448Z","iopub.execute_input":"2024-12-29T05:58:47.077874Z","iopub.status.idle":"2024-12-29T05:58:47.442420Z","shell.execute_reply.started":"2024-12-29T05:58:47.077835Z","shell.execute_reply":"2024-12-29T05:58:47.440676Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-aa83b746892f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /kaggle/input/fruits/fruits-360_dataset_100x100/fruits-360/Training"],"ename":"NotFoundError","evalue":"Could not find directory /kaggle/input/fruits/fruits-360_dataset_100x100/fruits-360/Training","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"test_generator = image_dataset_from_directory(test_dir,image_size=(224,224),batch_size =200,label_mode=\"categorical\")\ntest_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.442977Z","iopub.status.idle":"2024-12-29T05:58:47.443348Z","shell.execute_reply":"2024-12-29T05:58:47.443214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data processing, Data Augumentation\n### Model Building","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\n\nIMAGE_SIZE = [224, 224]  # we will keep the image size as (224,224). You can increase the size for better results.\n\n# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\nvgg = VGG19(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n\n# this will exclude the initial layers from training phase as there are already been trained.\nfor layer in vgg.layers:\n    layer.trainable = False\n\nx = Flatten()(vgg.output)\nx = Dense(4096, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\nx = Dense(1096, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\nx = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n\nmodel = Model(inputs = vgg.input, outputs = x)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.444056Z","iopub.status.idle":"2024-12-29T05:58:47.444484Z","shell.execute_reply":"2024-12-29T05:58:47.444301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training & Testing","metadata":{}},{"cell_type":"code","source":"history = model.fit(training_generator,\n                   steps_per_epoch = 10000,  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results.\n                   epochs = 1,  # change this for better results\n                   validation_data = test_generator,\n                   validation_steps = 3000)  # this should be equal to total number of images in validation set.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.445340Z","iopub.status.idle":"2024-12-29T05:58:47.445696Z","shell.execute_reply":"2024-12-29T05:58:47.445572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display Result","metadata":{}},{"cell_type":"code","source":"print ('Training Accuracy = ' + str(history.history['accuracy']))\nprint ('Validation Accuracy = ' + str(history.history['val_accuracy']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T05:58:47.446634Z","iopub.status.idle":"2024-12-29T05:58:47.447006Z","shell.execute_reply":"2024-12-29T05:58:47.446849Z"}},"outputs":[],"execution_count":null}]}